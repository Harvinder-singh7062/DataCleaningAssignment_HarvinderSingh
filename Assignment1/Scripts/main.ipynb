{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement data-preprocessor (from versions: none)\n",
      "ERROR: No matching distribution found for data-preprocessor\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_preprocessor'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata_preprocessor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdp\u001b[39;00m  \u001b[38;5;66;03m# This is your custom preprocessing module\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Step 1: Read the messy dataset from a CSV file\u001b[39;00m\n\u001b[32m     11\u001b[39m messy_data = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33m../Data/messy_data.csv\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'data_preprocessor'"
     ]
    }
   ],
   "source": [
    "# Install the data preprocessor module if needed (only for Jupyter or Colab environments)\n",
    "%pip install data-preprocessor\n",
    "\n",
    "# Bring in the tools we'll need\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import data_preprocessor as dp  # This is your custom preprocessing module\n",
    "\n",
    "# Step 1: Read the messy dataset from a CSV file\n",
    "messy_data = pd.read_csv('../Data/messy_data.csv')\n",
    "clean_data = messy_data.copy()  # Make a safe copy to clean\n",
    "\n",
    "# Step 2: Start cleaning the data\n",
    "clean_data = dp.impute_missing_values(clean_data, strategy='mean')  # Fill in missing values\n",
    "clean_data = dp.remove_duplicates(clean_data)                       # Get rid of duplicate rows\n",
    "clean_data = dp.normalize_data(clean_data)                          # Scale numerical values to a consistent range\n",
    "clean_data = dp.remove_redundant_features(clean_data)               # Drop columns that are too similar to each other\n",
    "\n",
    "# Step 3: (Optional) Save the clean version for future use\n",
    "# clean_data.to_csv('../Data/clean_data.csv', index=False)\n",
    "\n",
    "# Step 4: Train a logistic regression model and see how well it performs\n",
    "dp.simple_model(clean_data, print_report=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Identify columns that are missing >25% of the data and drop them (not the target).\\n2. Remove rows that are missing target values.\\n3. Remove duplicate rows.\\n4. Impute missing values in the remaining columns using mean for numerical and mode for categorical.\\n5. Normalize numerical features.\\n6. Remove redundant features (e.g., highly correlated features).\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data = messy_data.copy()\n",
    "\n",
    "'''\n",
    "1. Identify columns that are missing >25% of the data and drop them (not the target).\n",
    "2. Remove rows that are missing target values.\n",
    "3. Remove duplicate rows.\n",
    "4. Impute missing values in the remaining columns using mean for numerical and mode for categorical.\n",
    "5. Normalize numerical features.\n",
    "6. Remove redundant features (e.g., highly correlated features).\n",
    "'''\n",
    "# testing\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
